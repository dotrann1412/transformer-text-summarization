{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model by using the local path or the model name of a huggingface model.\n",
    "\n",
    "model =  T5ForConditionalGeneration.from_pretrained('ndtran/t5-small_cnn-daily-mails')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rouge-score, sentencepiece, spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    global model, tokenizer, device, configs, nlp\n",
    "    \n",
    "    input_ids = tokenizer(configs['task_prefix'] + text, return_tensors = 'pt').input_ids\n",
    "        \n",
    "    generated_ids = model.generate(\n",
    "        input_ids.to(device), \n",
    "        do_sample = True, \n",
    "        max_length = 256,\n",
    "        top_k = 1, \n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    doc = nlp(tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True))\n",
    "    sents = [str(sent).lstrip(string.punctuation + ' ').rstrip() for sent in doc.sents]\n",
    "    \n",
    "    for i, sent in enumerate(sents):\n",
    "        if len(sent) > 0:\n",
    "            sents[i] = sent[0].upper() + sent[1:]\n",
    "    \n",
    "    return \" \".join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multipart_summarize(text):\n",
    "    global model, tokenizer, device, configs, nlp\n",
    "    \n",
    "        \n",
    "    buffer, tokens_count = '', 0\n",
    "    nlp_text = nlp(text)\n",
    "    \n",
    "    blocks = []\n",
    "    \n",
    "    for sent in nlp_text.sents:\n",
    "        tokens = tokenizer.tokenize(str(sent))\n",
    "                \n",
    "        if len(tokens) > 512:\n",
    "            if tokens_count > 0:\n",
    "                blocks.append(buffer)\n",
    "                buffer, tokens_count = '', 0\n",
    "            \n",
    "            blocks.append(str(sent))\n",
    "            \n",
    "        buffer += str(sent)\n",
    "        tokens_count += len(tokens)\n",
    "        \n",
    "        if tokens_count > 512:\n",
    "            blocks.append(buffer)\n",
    "            buffer, tokens_count = '', 0\n",
    "            \n",
    "    if tokens_count > 0:\n",
    "        blocks.append(buffer)\n",
    "                    \n",
    "    return \" \".join(summarize(e) for e in blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to use the above functions\n",
    "\n",
    "with open('very-long-document.txt', 'r') as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "print(summarize(text)) \n",
    "# or\n",
    "print(multipart_summarize(text)) # recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "with open('/kaggle/input/t5-base-tokens-cnn-daily/test_ds_encoded.json', 'r') as fp:\n",
    "    test_list = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDaily(Dataset):\n",
    "    def __init__(self, elements):\n",
    "        self.elements = elements\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            res = self.elements[index]\n",
    "            return torch.LongTensor(res['input_ids']), torch.LongTensor(res['attention_mask']), torch.LongTensor(res['labels'])\n",
    "        except Exception as err:\n",
    "            print('Exception raised while loading item', index, '\\nTrying to load', (index + 1) % len(self.elements))\n",
    "            print(err)\n",
    "            return None # self.__getitem__((index + 1) % len(self.elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CNNDaily(test_list)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = 128,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get raw text from tokens\n",
    "def unpack(inputs_ids, labels_ids, outputs_ids):\n",
    "        \n",
    "    inputs = tokenizer.batch_decode(\n",
    "        inputs_ids, skip_special_tokens=True)\n",
    "    \n",
    "    labels_ids[labels_ids == -100] = 0\n",
    "    \n",
    "    labels = tokenizer.batch_decode(\n",
    "        labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(\n",
    "        outputs_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return [ {\n",
    "            'input': inputs[i],\n",
    "            'label': labels[i],\n",
    "            'output': outputs[i]\n",
    "        } for i in range(\n",
    "            min(inputs_ids.shape[0], labels_ids.shape[0], outputs_ids.shape[0])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] # store the dictionary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X, Y, Z) in tqdm(\n",
    "    enumerate(test_loader), total = len(test_loader), \n",
    "    unit = 'Batch', desc = 'Generating'\n",
    "):\n",
    "    X, Z = X.to(device), Z.to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        X, \n",
    "        do_sample=True, \n",
    "        max_length = 128, \n",
    "        top_k = 1, \n",
    "        temperature = 0.7\n",
    "    )\n",
    "    \n",
    "    results += unpack(X, Z, generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the last step\n",
    "\n",
    "- Save all the results to file to be easily used later for evaluation or analysis\n",
    "- Here is our sample output for the pre-trained model: [Link](https://www.kaggle.com/code/ndtran/t5-small-inference/notebook?scriptVersionId=135131238)\n",
    "- And the results for original model: [Link](https://www.kaggle.com/ndtran/t5-small-inference?scriptVersionId=135131482)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
